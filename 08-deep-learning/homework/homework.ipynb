{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9aad903a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b4287d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Output size = (Input size - Kernel size + 2 × Padding) / Stride + 1\n",
    "\n",
    "# Parameters:\n",
    "Input size = 200\n",
    "Kernel size = 3\n",
    "Padding = 0 (default)\n",
    "Stride = 1 (default)\n",
    "\n",
    "# Calculation:\n",
    "Output size = (200 - 3 + 2 × 0) / 1 + 1\n",
    "           = (200 - 3 + 0) / 1 + 1\n",
    "           = 197 / 1 + 1\n",
    "           = 197 + 1\n",
    "           = 198\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "155b91f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HairClassifier(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (relu1): ReLU()\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=313632, out_features=64, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class HairClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HairClassifier, self).__init__()\n",
    "        \n",
    "        # 1. Convolutional layer: 32 filters, 3x3 kernel\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=3,      # RGB input\n",
    "            out_channels=32,    # 32 filters\n",
    "            kernel_size=3       # 3x3 kernel\n",
    "        )\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        # 2. Max pooling layer: 2x2\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # 3. Flatten (implemented in forward method)\n",
    "        \n",
    "        # 4. Fully connected layer: 64 neurons\n",
    "        # Calculate input size:\n",
    "        # Input: (3, 200, 200)\n",
    "        # After Conv2d(3x3): (32, 198, 198)\n",
    "        # After MaxPool2d(2x2): (32, 99, 99)\n",
    "        # Flatten: 32 * 99 * 99 = 313,632\n",
    "        self.fc1 = nn.Linear(32 * 99 * 99, 64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        # 5. Output layer: 1 neuron (binary classification)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "        # Note: No sigmoid here because we use BCEWithLogitsLoss\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Conv + ReLU\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        # MaxPooling\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)  # or use torch.flatten(x, 1)\n",
    "        \n",
    "        # FC + ReLU\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu2(x)\n",
    "        \n",
    "        # Output\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create model\n",
    "model = HairClassifier()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87a9324",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57c80c2",
   "metadata": {},
   "source": [
    "Answer: nn.BCEWithLogitsLoss(), because BCEWithLogitsLoss = Sigmoid + Binary Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf6a93c",
   "metadata": {},
   "source": [
    "* nn.MSELoss() - Used for regression tasks\n",
    "* nn.CrossEntropyLoss() - Used for multi-class classification (requires multiple output neurons)\n",
    "* nn.CosineEmbeddingLoss() - Used for similarity learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0316bec",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5774cd07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 198, 198]             896\n",
      "              ReLU-2         [-1, 32, 198, 198]               0\n",
      "         MaxPool2d-3           [-1, 32, 99, 99]               0\n",
      "            Linear-4                   [-1, 64]      20,072,512\n",
      "              ReLU-5                   [-1, 64]               0\n",
      "            Linear-6                    [-1, 1]              65\n",
      "================================================================\n",
      "Total params: 20,073,473\n",
      "Trainable params: 20,073,473\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.46\n",
      "Forward/backward pass size (MB): 21.54\n",
      "Params size (MB): 76.57\n",
      "Estimated Total Size (MB): 98.57\n",
      "----------------------------------------------------------------\n",
      "Total parameters: 20073473\n"
     ]
    }
   ],
   "source": [
    "# Option 1: Using torchsummary (install with: pip install torchsummary)\n",
    "from torchsummary import summary\n",
    "summary(model, input_size=(3, 200, 200))\n",
    "\n",
    "# Option 2: Manual counting\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f670eca3",
   "metadata": {},
   "source": [
    "Answer: 20073473"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e88515b",
   "metadata": {},
   "source": [
    "### Generators and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f85a81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((200, 200)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(\n",
    "    root='./data/train',\n",
    "    transform=train_transforms\n",
    ")\n",
    "\n",
    "test_dataset = datasets.ImageFolder(\n",
    "    root='./data/test',\n",
    "    transform=train_transforms\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=20,\n",
    "    shuffle=True  \n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=20,\n",
    "    shuffle=False  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a79eafa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.6665, Acc: 0.6112, Val Loss: 0.6511, Val Acc: 0.6617\n",
      "Epoch 2/10, Loss: 0.5702, Acc: 0.6787, Val Loss: 0.6332, Val Acc: 0.6318\n",
      "Epoch 3/10, Loss: 0.5207, Acc: 0.7350, Val Loss: 0.6143, Val Acc: 0.6766\n",
      "Epoch 4/10, Loss: 0.4773, Acc: 0.7600, Val Loss: 0.6049, Val Acc: 0.6617\n",
      "Epoch 5/10, Loss: 0.4606, Acc: 0.7550, Val Loss: 0.7307, Val Acc: 0.5672\n",
      "Epoch 6/10, Loss: 0.3954, Acc: 0.8275, Val Loss: 0.6412, Val Acc: 0.6866\n",
      "Epoch 7/10, Loss: 0.2844, Acc: 0.8838, Val Loss: 0.8307, Val Acc: 0.6816\n",
      "Epoch 8/10, Loss: 0.2885, Acc: 0.8788, Val Loss: 0.7052, Val Acc: 0.7114\n",
      "Epoch 9/10, Loss: 0.1882, Acc: 0.9313, Val Loss: 0.9275, Val Acc: 0.6866\n",
      "Epoch 10/10, Loss: 0.2585, Acc: 0.8912, Val Loss: 0.8158, Val Acc: 0.6915\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.002, momentum=0.8)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "num_epochs = 10\n",
    "history = {'acc': [], 'loss': [], 'val_acc': [], 'val_loss': []}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        labels = labels.float().unsqueeze(1) \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    epoch_acc = correct_train / total_train\n",
    "    history['loss'].append(epoch_loss)\n",
    "    history['acc'].append(epoch_acc)\n",
    "    \n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            labels = labels.float().unsqueeze(1)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_running_loss += loss.item() * images.size(0)\n",
    "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_epoch_loss = val_running_loss / len(test_dataset)\n",
    "    val_epoch_acc = correct_val / total_val\n",
    "    history['val_loss'].append(val_epoch_loss)\n",
    "    history['val_acc'].append(val_epoch_acc)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "          f\"Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}, \"\n",
    "          f\"Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad25790",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "200bd821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median training accuracy: 0.79\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "median_train_acc = np.median(history['acc'])\n",
    "print(f\"Median training accuracy: {median_train_acc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb1739c",
   "metadata": {},
   "source": [
    "Answer: 0.84, closest one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63c76ba",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b0694e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Std of training loss: 0.146\n"
     ]
    }
   ],
   "source": [
    "std_train_loss = np.std(history['loss'])\n",
    "print(f\"Std of training loss: {std_train_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebb674a",
   "metadata": {},
   "source": [
    "Answer: 0.171, the closest one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b74c9c",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d40ec34",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms_aug = transforms.Compose([\n",
    "    transforms.Resize((200, 200)),\n",
    "    transforms.RandomRotation(50),  \n",
    "    transforms.RandomResizedCrop(200, scale=(0.9, 1.0), ratio=(0.9, 1.1)), \n",
    "    transforms.RandomHorizontalFlip(), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "train_dataset_aug = datasets.ImageFolder(\n",
    "    root='./data/train',\n",
    "    transform=train_transforms_aug\n",
    ")\n",
    "\n",
    "train_loader_aug = DataLoader(\n",
    "    train_dataset_aug,\n",
    "    batch_size=20,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5266ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20, Loss: 0.6472, Acc: 0.6538, Val Loss: 0.6916, Val Acc: 0.7114\n",
      "Epoch 12/20, Loss: 0.6190, Acc: 0.6475, Val Loss: 0.6385, Val Acc: 0.6368\n",
      "Epoch 13/20, Loss: 0.5847, Acc: 0.6837, Val Loss: 0.6113, Val Acc: 0.6667\n",
      "Epoch 14/20, Loss: 0.5550, Acc: 0.7137, Val Loss: 0.5822, Val Acc: 0.6965\n",
      "Epoch 15/20, Loss: 0.5496, Acc: 0.7125, Val Loss: 0.5541, Val Acc: 0.6915\n",
      "Epoch 16/20, Loss: 0.5193, Acc: 0.7212, Val Loss: 0.6146, Val Acc: 0.6816\n",
      "Epoch 17/20, Loss: 0.5227, Acc: 0.7312, Val Loss: 0.5665, Val Acc: 0.6965\n",
      "Epoch 18/20, Loss: 0.5223, Acc: 0.7175, Val Loss: 0.6257, Val Acc: 0.6617\n",
      "Epoch 19/20, Loss: 0.5166, Acc: 0.7412, Val Loss: 0.5468, Val Acc: 0.7114\n",
      "Epoch 20/20, Loss: 0.5142, Acc: 0.7462, Val Loss: 0.5622, Val Acc: 0.7214\n"
     ]
    }
   ],
   "source": [
    "# Continue training for 10 more epochs (Epoch 11-20) with augmentation\n",
    "num_epochs = 10  # Train for 10 more epochs\n",
    "\n",
    "for epoch in range(num_epochs):  # This will be epochs 0-9 in this loop\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    for images, labels in train_loader_aug:  # Use augmented data loader\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        labels = labels.float().unsqueeze(1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_dataset_aug)  # Use augmented dataset length\n",
    "    epoch_acc = correct_train / total_train\n",
    "    history['loss'].append(epoch_loss)\n",
    "    history['acc'].append(epoch_acc)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            labels = labels.float().unsqueeze(1)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_running_loss += loss.item() * images.size(0)\n",
    "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_epoch_loss = val_running_loss / len(test_dataset)\n",
    "    val_epoch_acc = correct_val / total_val\n",
    "    history['val_loss'].append(val_epoch_loss)\n",
    "    history['val_acc'].append(val_epoch_acc)\n",
    "    \n",
    "    # Print with correct epoch number (11-20)\n",
    "    actual_epoch = len(history['loss'])  # Total epochs so far\n",
    "    print(f\"Epoch {actual_epoch}/20, \"\n",
    "          f\"Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}, \"\n",
    "          f\"Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08eaaa90",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4892d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean val loss (epochs 11-20): 0.60\n"
     ]
    }
   ],
   "source": [
    "mean_val_loss = np.mean(history['val_loss'][10:20])\n",
    "print(f\"Mean val loss (epochs 11-20): {mean_val_loss:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c01d678",
   "metadata": {},
   "source": [
    "Answer: 0.88, the closest one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57be7cb7",
   "metadata": {},
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dccab4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean val acc (last 5 epochs): 0.69\n"
     ]
    }
   ],
   "source": [
    "mean_val_acc_last5 = np.mean(history['val_acc'][15:20])\n",
    "print(f\"Mean val acc (last 5 epochs): {mean_val_acc_last5:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1574aa",
   "metadata": {},
   "source": [
    "Answer: 0.68, the closest one"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
